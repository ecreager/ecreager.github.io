<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | Elliot  Creager</title>
    <meta name="author" content="Elliot  Creager">
    <meta name="description" content="Conference and workshop papers listed in reversed chronological order. * denotes equal contribution">
    <meta name="keywords" content="machine-learning, artificial-intelligence, research">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%86&lt;/text&gt;&lt;/svg&gt;">
    
    <!-- <link rel="stylesheet" href="/assets/css/main.css"> -->
    <link rel="stylesheet" href="https://ecreager.github.io//assets/css/main.css">
    <link rel="canonical" href="https://ecreager.github.io//publications/">
    
    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- <a class="navbar-brand title font-weight-lighter" href="/"> -->
          <a class="navbar-brand title font-weight-lighter" href="https://ecreager.github.io//"><span class="font-weight-bold">Elliot </span>Creager</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <!-- <a class="nav-link" href="/">about -->
                <a class="nav-link" href="https://ecreager.github.io//">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <!-- <a class="nav-link" href="/publications/">publications -->
                <!-- <a class="nav-link" href="">publications -->
                <!-- <a class="nav-link" href="https://ecreager.github.io//publications/">publications -->
                <a class="nav-link" href="https://ecreager.github.io//publications/">publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <!-- <a class="nav-link" href="/teaching/">teaching -->
                <!-- <a class="nav-link" href="">teaching -->
                <!-- <a class="nav-link" href="https://ecreager.github.io//publications/">teaching -->
                <a class="nav-link" href="https://ecreager.github.io//teaching/">teaching</a>
              </li>
              <li class="nav-item ">
                <!-- <a class="nav-link" href="/join/">openings -->
                <!-- <a class="nav-link" href="">openings -->
                <!-- <a class="nav-link" href="https://ecreager.github.io//publications/">openings -->
                <a class="nav-link" href="https://ecreager.github.io//join/">openings</a>
              </li>
              <li class="nav-item ">
                <!-- <a class="nav-link" href="/bio/">bio -->
                <!-- <a class="nav-link" href="">bio -->
                <!-- <a class="nav-link" href="https://ecreager.github.io//publications/">bio -->
                <a class="nav-link" href="https://ecreager.github.io//bio/">bio</a>
              </li>
              <li class="nav-item ">
                <!-- <a class="nav-link" href="/cv.html">cv -->
                <!-- <a class="nav-link" href="">cv -->
                <!-- <a class="nav-link" href="https://ecreager.github.io//publications/">cv -->
                <a class="nav-link" href="https://ecreager.github.io//cv.html">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description">Conference and workshop papers listed in reversed chronological order. * denotes equal contribution</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2025</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="Chataigner2025say" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework</div>
          <!-- Author -->
          <div class="author">
          

          Cléa Chataigner, Rebecca Ma, Prakhar Ganesh, Afaf Taïk, <em>Elliot Creager</em>, and Golnoosh Farnadi</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2505.03563" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2505.03563" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Large language models (LLMs) are sensitive to subtle changes in prompt phrasing, complicating efforts to audit them reliably. Prior approaches often rely on arbitrary or ungrounded prompt variations, which may miss key linguistic and demographic factors in real-world usage. We introduce AUGMENT (Automated User-Grounded Modeling and Evaluation of Natural Language Transformations), a framework for systematically generating and evaluating controlled, realistic prompt paraphrases based on linguistic structure and user demographics. AUGMENT ensures paraphrase quality through a combination of semantic, stylistic, and instruction-following criteria. In a case study on the BBQ dataset, we show that user-grounded paraphrasing leads to significant shifts in LLM performance and bias metrics across nine models. Our findings highlight the need for more representative and structured approaches to prompt variation in LLM auditing.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@unpublished</span><span class="p">{</span><span class="nl">Chataigner2025say</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chataigner, Cléa and Ma, Rebecca and Ganesh, Prakhar and Taïk, Afaf and Creager, Elliot and Farnadi, Golnoosh}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2505.03563}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2505.03563}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="Solanki2025crowding" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy</div>
          <!-- Author -->
          <div class="author">
          

          Rushabh Solanki, Meghana Bhange, Ulrich Aïvodji, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2505.05707" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2505.05707" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The integration of AI into daily life has generated considerable attention and excitement, while also raising concerns about automating algorithmic harms and re-entrenching existing social inequities. While the responsible deployment of trustworthy AI systems is a worthy goal, there are many possible ways to realize it, from policy and regulation to improved algorithm design and evaluation. In fact, since AI trains on social data, there is even a possibility for everyday users, citizens, or workers to directly steer its behavior through Algorithmic Collective Action, by deliberately modifying the data they share with a platform to drive its learning process in their favor. This paper considers how these grassroots efforts to influence AI interact with methods already used by AI firms and governments to improve model trustworthiness. In particular, we focus on the setting where the AI firm deploys a differentially private model, motivated by the growing regulatory focus on privacy and data protection. We investigate how the use of Differentially Private Stochastic Gradient Descent (DPSGD) affects the collective’s ability to influence the learning process. Our findings show that while differential privacy contributes to the protection of individual data, it introduces challenges for effective algorithmic collective action. We characterize lower bounds on the success of algorithmic collective action under differential privacy as a function of the collective’s size and the firm’s privacy parameters, and verify these trends experimentally by simulating collective action during the training of deep neural network classifiers across several datasets.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@unpublished</span><span class="p">{</span><span class="nl">Solanki2025crowding</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Solanki, Rushabh and Bhange, Meghana and Aïvodji, Ulrich and Creager, Elliot}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2505.05707}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2505.05707}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="Hill2025transformers" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Transformers Don’t In-Context Learn Least Squares Regression</div>
          <!-- Author -->
          <div class="author">
          

          Joshua Hill, Ben Eyre, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In ICML 2025 Workshop on Reliable and Responsible Foundation Models</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In-context learning (ICL) has emerged as a powerful capability of large pretrained transformers, enabling them to solve new tasks implicit in example input–output pairs without any gradient updates. Despite its practical success, the mechanisms underlying ICL remain largely mysterious. In this work we study synthetic linear regression to probe how transformers implement learning at inference time. Previous works have demonstrated that transformers match the performance of learning rules such as Ordinary Least Squares (OLS) regression or gradient descent and have suggested ICL is facilitated in transformers through the learned implementation of one of these techniques. In this work, we demonstrate through a suite of out‑of‑distribution generalization experiments that transformers trained for ICL fail to generalize after shifts in the prompt distribution, a behaviour that is inconsistent with the notion of transformers implementing algorithms such as OLS.  Finally, we highlight the role of the pretraining corpus in shaping ICL behaviour through a spectral analysis of the learned representations in the residual stream. Inputs from the same distribution as the training data produce representations with a unique \textitspectral signature: inputs from this distribution tend to have the same top two singular vectors. This spectral signature is not shared by out-of-distribution inputs, and a metric characterizing the presence of this signature is highly correlated with low loss.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Hill2025transformers</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hill, Joshua and Eyre, Ben and Creager, Elliot}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transformers Don't In-Context Learn Least Squares Regression}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML 2025 Workshop on Reliable and Responsible Foundation Models}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{forthcoming}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IEEE ETHICS (Student Oral)</abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2025out" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Out of Sight, Out of Mind: Problems Perpetuated by Training on Facial Images Collected by De Facto Consent</div>
          <!-- Author -->
          <div class="author">
          

          Chloe Nguyen, Jin Sol Kim, Lai-Tze Fan, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In IEEE ETHICS 2025 Undergraduate Student Track</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Images of human faces are being shared online at an unprecedented scale. These images—rich in biometric data—feed the datasets that power facial recognition technologies (FRT), such as airport security and facial identification on smartphones. But how this data is collected is increasingly opaque and invasive. Over the past year, major social media platforms, such as X and LinkedIn, have adopted de facto consent policies—that is to say, the automated permissions of user consent—allowing their machine learning models to train on user data without meaningful or even conscious user input. Public concern tends to emerge only after harm becomes visible, by which point the user’s biometric information has already shaped the system’s behavior.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nguyen2025out</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Chloe and Kim, Jin Sol and Fan, Lai-Tze and Creager, Elliot}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Out of Sight, Out of Mind: Problems Perpetuated by Training on Facial Images Collected by De Facto Consent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE ETHICS 2025 Undergraduate Student Track}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{\textbf{Oral}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CPI GranConf (Spotlight)</abbr></div>

        <!-- Entry bib key -->
        <div id="solanki2025algorithmic" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Algorithmic Collective Action Directed at Fair and Private Learning Algorithms</div>
          <!-- Author -->
          <div class="author">
          

          Rushabh Solanki, Meghana Bhange, Ulrich Aïvodji, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In University of Waterloo Cybersecurity and Privacy Institute Graduate Student Conference</em> 2025
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://uwaterloo.ca/cybersecurity-privacy-institute/sites/default/files/uploads/documents/poster-59.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">solanki2025algorithmic</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Solanki, Rushabh and Bhange, Meghana and Aïvodji, Ulrich and Creager, Elliot}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Algorithmic Collective Action Directed at Fair and Private Learning Algorithms}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{University of Waterloo Cybersecurity and Privacy Institute Graduate Student Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{\textbf{Spotlight}}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="solanki2024promoting" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Promoting User Data Autonomy During the Dissolution of a Monopolistic Firm</div>
          <!-- Author -->
          <div class="author">
          

          Rushabh Solanki, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In NeurIPS 2024 Workshop on Regulatable ML</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2411.13546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2411.13546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The deployment of AI in consumer products is currently focused on the use of so-called foundation models, large neural networks pre-trained on massive corpora of digital records. This emphasis on scaling up datasets and pre-training computation raises the risk of further consolidating the industry, and enabling monopolistic (or oligopolistic) behavior. Judges and regulators seeking to improve market competition may employ various remedies. This paper explores dissolution – the breaking up of a monopolistic entity into smaller firms – as one such remedy, focusing in particular on the technical challenges and opportunities involved in the breaking up of large models and datasets. We show how the framework of Conscious Data Contribution can enable user autonomy during under dissolution. Through a simulation study, we explore how fine-tuning and the phenomenon of "catastrophic forgetting" could actually prove beneficial as a type of machine unlearning that allows users to specify which data they want used for what purposes.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">solanki2024promoting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Promoting User Data Autonomy During the Dissolution of a Monopolistic Firm}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Solanki, Rushabh and Creager, Elliot}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2411.13546}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS 2024 Workshop on Regulatable ML}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="jaipersaud2024show" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Show, Don’t Tell: Uncovering Implicit Character Portrayal using LLMs</div>
          <!-- Author -->
          <div class="author">
          

          Brandon Jaipersaud, Zining Zhu, Frank Rudzicz, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In NeurIPS 2024 Workshop on Creativity and Generative AI</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2412.04576" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2412.04576" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Tools for analyzing character portrayal in fiction are valuable for writers and literary scholars in developing and interpreting compelling stories. Existing tools, such as visualization tools for analyzing fictional characters, primarily rely on explicit textual indicators of character attributes. However, portrayal is often implicit, revealed through actions and behaviors rather than explicit statements. We address this gap by leveraging large language models (LLMs) to uncover implicit character portrayals. We start by generating a dataset for this task with greater cross-topic similarity, lexical diversity, and narrative lengths than existing narrative text corpora such as TinyStories and WritingPrompts. We then introduce LIIPA (LLMs for Inferring Implicit Portrayal for Character Analysis), a framework for prompting LLMs to uncover character portrayals. LIIPA can be configured to use various types of intermediate computation (character attribute word lists, chain-of-thought) to infer how fictional characters are portrayed in the source text. We find that LIIPA outperforms existing approaches, and is more robust to increasing character counts (number of unique persons depicted) due to its ability to utilize full narrative context. Lastly, we investigate the sensitivity of portrayal estimates to character demographics, identifying a fairness-accuracy tradeoff among methods in our LIIPA framework – a phenomenon familiar within the algorithmic fairness literature. Despite this tradeoff, all LIIPA variants consistently outperform non-LLM baselines in both fairness and accuracy. Our work demonstrates the potential benefits of using LLMs to analyze complex characters and to better understand how implicit portrayal biases may manifest in narrative texts.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jaipersaud2024show</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jaipersaud, Brandon and Zhu, Zining and Rudzicz, Frank and Creager, Elliot}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2412.04576}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS 2024 Workshop on Creativity and Generative AI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="eyre_out_2024" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Out of the Ordinary: Robust Regression by Spectral Adaptation</div>
          <!-- Author -->
          <div class="author">
          

          Benjamin Eyre, <em>Elliot Creager</em>, David Madras, Vardan Papyan, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 41st International Conference on Machine Learning</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2312.17463" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2312.17463" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Designing deep neural network classifiers that perform robustly on distributions differing from the available training data is an active area of machine learning research. However, out-of-distribution generalization for regression-the analogous problem for modeling continuous targets-remains relatively unexplored. To tackle this problem, we return to first principles and analyze how the closed-form solution for Ordinary Least Squares (OLS) regression is sensitive to covariate shift. We characterize the out-of-distribution risk of the OLS model in terms of the eigenspectrum decomposition of the source and target data. We then use this insight to propose a method for adapting the weights of the last layer of a pre-trained neural regression model to perform better on input data originating from a different distribution. We demonstrate how this lightweight spectral adaptation procedure can improve out-of-distribution performance for synthetic and real-world datasets.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eyre_out_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Out of the {Ordinary}: {Robust} {Regression} by {Spectral} {Adaptation}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v235/eyre24a.html}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 41st {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eyre, Benjamin and Creager, Elliot and Madras, David and Papyan, Vardan and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="alamdari_remembering_2024" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making</div>
          <!-- Author -->
          <div class="author">
          

          Parand A. Alamdari, Toryn Q. Klassen, <em>Elliot Creager</em>, and Sheila A. Mcilraith</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 41st International Conference on Machine Learning</em> Jul 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2312.04772" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2312.04772" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Fair decision making has largely been studied with respect to a single decision. Here we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points \textlessem\textgreaterwithin\textless/em\textgreater the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We explore the interplay between non-Markovian fairness and memory and how memory can support construction of fair policies. Finally, we introduce the FairQCM algorithm, which can automatically augment its training data to improve sample efficiency in the synthesis of fair policies via reinforcement learning.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">alamdari_remembering_2024</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of {Machine} {Learning} {Research}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Remembering to {Be} {Fair}: {Non}-{Markovian} {Fairness} in {Sequential} {Decision} {Making}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{235}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v235/alamdari24a.html}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 41st {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alamdari, Parand A. and Klassen, Toryn Q. and Creager, Elliot and Mcilraith, Sheila A.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{906--920}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Thesis</abbr></div>

        <!-- Entry bib key -->
        <div id="creager_robust_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Robust Machine Learning by Transforming and Augmenting Imperfect Training Data</div>
          <!-- Author -->
          <div class="author">
          

          <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            Jul 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2312.12597" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2312.12597" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Machine Learning (ML) is an expressive framework for turning data into computer programs. Across many problem domains – both in industry and policy settings – the types of computer programs needed for accurate prediction or optimal control are difficult to write by hand. On the other hand, collecting instances of desired system behavior may be relatively more feasible. This makes ML broadly appealing, but also induces data sensitivities that often manifest as unexpected failure modes during deployment. In this sense, the training data available tend to be imperfect for the task at hand. This thesis explores several data sensitivities of modern machine learning and how to address them. We begin by discussing how to prevent ML from codifying prior human discrimination measured in the training data, where we take a fair representation learning approach. We then discuss the problem of learning from data containing spurious features, which provide predictive fidelity during training but are unreliable upon deployment. Here we observe that insofar as standard training methods tend to learn such features, this propensity can be leveraged to search for partitions of training data that expose this inconsistency, ultimately promoting learning algorithms invariant to spurious features. Finally, we turn our attention to reinforcement learning from data with insufficient coverage over all possible states and actions. To address the coverage issue, we discuss how causal priors can be used to model the single-step dynamics of the setting where data are collected. This enables a new type of data augmentation where observed trajectories are stitched together to produce new but plausible counterfactual trajectories.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">creager_robust_2023</span><span class="p">,</span>
  <span class="na">type</span> <span class="p">=</span> <span class="s">{{PhD} {Thesis}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust {Machine} {Learning} by {Transforming} and {Augmenting} {Imperfect} {Training} {Data}}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{University of Toronto (Canada)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Creager, Elliot}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICCV</abbr></div>

        <!-- Entry bib key -->
        <div id="mani_surfsup_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">SURFSUP: Learning Fluid Simulation for Novel Surfaces</div>
          <!-- Author -->
          <div class="author">
          

          Arjun Mani, Ishaan Preetam Chandratreya, <em>Elliot Creager</em>, Carl Vondrick, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In International Conference on Computer Vision</em> Oct 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2304.06197" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="http://arxiv.org/pdf/2304.06197" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/cvlab-columbia/surfsup" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://surfsup.cs.columbia.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Modeling the mechanics of fluid in complex scenes is vital to applications in design, graphics, and robotics. Learning-based methods provide fast and differentiable fluid simulators, however most prior work is unable to accurately model how fluids interact with genuinely novel surfaces not seen during training. We introduce SURFSUP, a framework that represents objects implicitly using signed distance functions (SDFs), rather than an explicit representation of meshes or particles. This continuous representation of geometry enables more accurate simulation of fluid-object interactions over long time periods while simultaneously making computation more efficient. Moreover, SURFSUP trained on simple shape primitives generalizes considerably out-of-distribution, even to complex real-world scenes and objects. Finally, we show we can invert our model to design simple objects to manipulate fluid flow.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mani_surfsup_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{SURFSUP}: {Learning} {Fluid} {Simulation} for {Novel} {Surfaces}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{SURFSUP}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2304.06197}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2023-04-14}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mani, Arjun and Chandratreya, Ishaan Preetam and Creager, Elliot and Vondrick, Carl and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{arXiv:2304.06197 [physics]}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Machine Learning, Physics - Fluid Dynamics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="pitis_mocoda_2022" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">MoCoDA: Model-based Counterfactual Data Augmentation</div>
          <!-- Author -->
          <div class="author">
          

          Silviu Pitis, <em>Elliot Creager</em>, Ajay Mandlekar, and Animesh Garg</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> Nov 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2210.11287" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="http://arxiv.org/pdf/2210.11287" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/spitis/mocoda" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The number of states in a dynamic process is exponential in the number of objects, making reinforcement learning (RL) difficult in complex, multi-object domains. For agents to scale to the real world, they will need to react to and reason about unseen combinations of objects. We argue that the ability to recognize and use local factorization in transition dynamics is a key element in unlocking the power of multi-object reasoning. To this end, we show that (1) known local structure in the environment transitions is sufficient for an exponential reduction in the sample complexity of training a dynamics model, and (2) a locally factored dynamics model provably generalizes out-of-distribution to unseen states and actions. Knowing the local structure also allows us to predict which unseen states and actions this dynamics model will generalize to. We propose to leverage these observations in a novel Model-based Counterfactual Data Augmentation (MoCoDA) framework. MoCoDA applies a learned locally factored dynamics model to an augmented distribution of states and actions to generate counterfactual transitions for RL. MoCoDA works with a broader set of local structures than prior work and allows for direct control over the augmented training distribution. We show that MoCoDA enables RL agents to learn policies that generalize to unseen states and actions. We use MoCoDA to train an offline RL agent to solve an out-of-distribution robotics manipulation task on which standard offline RL algorithms fail.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pitis_mocoda_2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{MoCoDA}: {Model}-based {Counterfactual} {Data} {Augmentation}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{MoCoDA}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2210.11287}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-10-26}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pitis, Silviu and Creager, Elliot and Mandlekar, Ajay and Garg, Animesh}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="eyre_towards_2022" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Towards Environment-Invariant Representation Learning for Robust Task Transfer</div>
          <!-- Author -->
          <div class="author">
          

          Benjamin Eyre, Richard Zemel, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In ICML Workshop on Spurious Correlation, Invariance, and Stability</em> Jul 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openreview.net/pdf?id=c4l4HoM2AFf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>To train a classification model that is robust to distribution shifts upon deployment, auxiliary labels indicating the various “environments” of data collection can be leveraged to mitigate reliance on environment-specific features. In this paper we attempt to determine where in the network the environment invariance property can be located for such a model, with the hopes of adapting a single pre-trained invariant model for use in multiple tasks. We discuss how to evaluate whether a model has formed an environment-invariant internal representation—as opposed to an invariant final classifier function—and propose an objective that encourages learning such a representation. We also extend color-biased digit recognition to a transfer setting where the target task requires an invariant model, but lacks the environment labels needed to train an invariant model from scratch, thus motivating the transfer of an invariant representation trained on a source task with environment labels.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eyre_towards_2022</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards {Environment}-{Invariant} {Representation} {Learning} for {Robust} {Task} {Transfer}}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eyre, Benjamin and Zemel, Richard and Creager, Elliot}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML Workshop on Spurious Correlation, Invariance, and Stability}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="creager_environment_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Environment Inference for Invariant Learning</div>
          <!-- Author -->
          <div class="author">
          

          <em>Elliot Creager</em>, Joern-Henrik Jacobsen, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 38th International Conference on Machine Learning</em> Jul 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2010.07249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2010.07249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/ecreager/eiil" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into “domains” or “environments”. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds dataset. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">creager_environment_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Environment {Inference} for {Invariant} {Learning}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v139/creager21a.html}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 38th {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Creager, Elliot and Jacobsen, Joern-Henrik and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{ISSN: 2640-3498}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2189--2200}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML (Oral)</abbr></div>

        <!-- Entry bib key -->
        <div id="trauble_disentangled_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On Disentangled Representations Learned from Correlated Data</div>
          <!-- Author -->
          <div class="author">
          

          Frederik Träuble, <em>Elliot Creager</em>, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Schölkopf, and Stefan Bauer</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 38th International Conference on Machine Learning</em> Jul 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2006.07886" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2006.07886" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/ftraeuble/disentanglement_lib" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">trauble_disentangled_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On {Disentangled} {Representations} {Learned} from {Correlated} {Data}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v139/trauble21a.html}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 38th {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{ISSN: 2640-3498}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10401--10412}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="dickson_measuring_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Measuring User Recourse in a Dynamic Recommender System</div>
          <!-- Author -->
          <div class="author">
          

          Dilys Dickson, and <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In ICML Workshop on Algorithmic Recourse</em> Jul 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://drive.google.com/file/d/10OwIV5g8XAxj3an5-0cJpaciVZEimNaw/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>From online searches to suggested videos in social media, recommendation systems are heavily relied upon to mediate access to digital information. Concerns have been raised about these systems over their potential for feedback loops that can create unintended consequences such as echochambers, ﬁlter bubbles and polarization in the digital space. In this paper, we measure the effect of prolonged exposure to recommendation on availability of diverse suggested content to the user. We use the deﬁnition of reachability (or user recourse) of Dean et al. (2020b), as the proportion of unseen items that could be recommended to the user in the future, which can be approximated using knowledge of the embedding space geometry for linear recommenders. Whereas previous work assumed a static recommender, we study the case where the recommender can change over time, either by training for longer given a ﬁxed dataset, or dynamically updating its training online through interactions with users. We ﬁnd that dynamic changes to the recommender system do indeed affect the recourse available to users.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="creager_online_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Online Algorithmic Recourse by Collective Action</div>
          <!-- Author -->
          <div class="author">
          

          <em>Elliot Creager</em>, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In ICML Workshop on Algorithmic Recourse</em> Jul 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2401.00055" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="https://arxiv.org/pdf/2401.00055" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Research on algorithmic recourse typically considers how an individual can reasonably change an unfavorable automated decision when interacting with a ﬁxed decision-making system. This paper focuses instead on the online setting, where system parameters are updated dynamically according to interactions with data subjects. Beyond the typical individual-level recourse, the online setting opens up new ways for groups to shape system decisions by leveraging the parameter update rule. We show empirically that recourse can be improved when users coordinate by jointly computing their feature perturbations, underscoring the importance of collective action in mitigating adverse automated decisions.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="pitis_counterfactual_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Counterfactual Data Augmentation using Locally Factored Dynamics</div>
          <!-- Author -->
          <div class="author">
          

          Silviu Pitis, <em>Elliot Creager</em>, and Animesh Garg</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> Dec 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2007.02863" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2007.02863" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/spitis/genrl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent\vphantom{} causal mechanisms. Such local causal structures can be leveraged to improve the sample efficiency of sequence prediction and off-policy reinforcement learning. We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We find that CoDA significantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings. Code available at https://github.com/spitis/mrl.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pitis_counterfactual_2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Counterfactual {Data} {Augmentation} using {Locally} {Factored} {Dynamics}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper/2020/hash/294e09f267683c7ddc6cc5134a7e68a8-Abstract.html}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in {Neural} {Information} {Processing} {Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pitis, Silviu and Creager, Elliot and Garg, Animesh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3976--3990}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="adragna_fairness_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Fairness and Robustness in Invariant Learning: A Case Study in Toxicity Classification</div>
          <!-- Author -->
          <div class="author">
          

          Robert Adragna, <em>Elliot Creager</em>, David Madras, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In NeurIPS 2020 Workshop on Fairness Through the Lens of Causality</em> Dec 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2011.06485" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2011.06485" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/adragnar/irm-toxicity-classification" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robustness is of central importance in machine learning and has given rise to the fields of domain generalization and invariant learning, which are concerned with improving performance on a test distribution distinct from but related to the training distribution. In light of recent work suggesting an intimate connection between fairness and robustness, we investigate whether algorithms from robust ML can be used to improve the fairness of classifiers that are trained on biased data and tested on unbiased data. We apply Invariant Risk Minimization (IRM), a domain generalization algorithm that employs a causal discovery inspired method to find robust predictors, to the task of fairly predicting the toxicity of internet comments. We show that IRM achieves better out-of-distribution accuracy and fairness than Empirical Risk Minimization (ERM) methods, and analyze both the difficulties that arise when applying IRM in practice and the conditions under which IRM will likely be effective in this scenario. We hope that this work will inspire further studies of how robust machine learning methods relate to algorithmic fairness.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">adragna_fairness_2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fairness and {Robustness} in {Invariant} {Learning}: {A} {Case} {Study} in {Toxicity} {Classification}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Fairness and {Robustness} in {Invariant} {Learning}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS 2020 Workshop on Fairness Through the Lens of Causality}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2011.06485}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Adragna, Robert and Creager, Elliot and Madras, David and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{arXiv:2011.06485 [cs]}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="creager_causal_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Causal Modeling for Fairness In Dynamical Systems</div>
          <!-- Author -->
          <div class="author">
          

          <em>Elliot Creager</em>, David Madras, Toniann Pitassi, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 37th International Conference on Machine Learning</em> Jul 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/1909.09141" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/1909.09141" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/ecreager/causal-dyna-fair" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In many applications areas—lending, education, and online recommenders, for example—fairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where sound causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and estimation by adjustment (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">creager_causal_2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Causal {Modeling} for {Fairness} {In} {Dynamical} {Systems}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v119/creager20a.html}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 37th {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Creager, Elliot and Madras, David and Pitassi, Toniann and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{ISSN: 2640-3498}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2185--2195}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="mladenov_optimizing_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach</div>
          <!-- Author -->
          <div class="author">
          

          Martin Mladenov, <em>Elliot Creager</em>, Omer Ben-Porat, Kevin Swersky, Richard Zemel, and Craig Boutilier</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 37th International Conference on Machine Learning</em> Jul 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2008.00104" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2008.00104" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Most recommender systems (RS) research assumes that a user’s utility can be maximized independently of the utility of the other agents (e.g., other users, content providers). In realistic settings, this is often not true – the dynamics of an RS ecosystem couple the long-term utility of all agents. In this work, we explore settings in which content providers cannot remain viable unless they receive a certain level of user engagement. We formulate this problem as one of equilibrium selection in the induced dynamical system, and show that it can be solved as an optimal constrained matching problem. Our model ensures the system reaches an equilibrium with maximal social welfare supported by a sufficiently diverse set of viable providers. We demonstrate that even in a simple, stylized dynamical RS model, the standard myopic approach to recommendation - always matching a user to the best provider - performs poorly. We develop several scalable techniques to solve the matching problem, and also draw connections to various notions of user regret and fairness, arguing that these outcomes are fairer in a utilitarian sense.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mladenov_optimizing_2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimizing {Long}-term {Social} {Welfare} in {Recommender} {Systems}: {A} {Constrained} {Matching} {Approach}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Optimizing {Long}-term {Social} {Welfare} in {Recommender} {Systems}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v119/mladenov20a.html}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 37th {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mladenov, Martin and Creager, Elliot and Ben-Porat, Omer and Swersky, Kevin and Zemel, Richard and Boutilier, Craig}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{ISSN: 2640-3498}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6987--6998}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="creager_flexibly_2019" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Flexibly Fair Representation Learning by Disentanglement</div>
          <!-- Author -->
          <div class="author">
          

          <em>Elliot Creager</em>, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 36th International Conference on Machine Learning</em> Jun 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/1906.02589" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/1906.02589" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also flexibly fair, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder—which does not require the sensitive attributes for inference—allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">creager_flexibly_2019</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flexibly {Fair} {Representation} {Learning} by {Disentanglement}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v97/creager19a.html}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 36th {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Creager, Elliot and Madras, David and Jacobsen, Joern-Henrik and Weis, Marissa and Swersky, Kevin and Pitassi, Toniann and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{ISSN: 2640-3498}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1436--1445}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div>

        <!-- Entry bib key -->
        <div id="chang_explaining_2019" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Explaining Image Classifiers by Counterfactual Generation</div>
          <!-- Author -->
          <div class="author">
          

          Chun-Hao Chang, <em>Elliot Creager</em>, Anna Goldenberg, and David Duvenaud</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In International Conference on Learning Representations</em> May 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/1807.08024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="http://arxiv.org/pdf/1807.08024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zzzace2000/FIDO-saliency" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren’t. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier’s decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chang_explaining_2019</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explaining {Image} {Classifiers} by {Counterfactual} {Generation}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/1807.08024}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chang, Chun-Hao and Creager, Elliot and Goldenberg, Anna and Duvenaud, David}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{arXiv:1807.08024 [cs]}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Computer Vision and Pattern Recognition}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACM-FAccT</abbr></div>

        <!-- Entry bib key -->
        <div id="madras_fairness_2019" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Fairness Through Causal Awareness: Learning Latent-Variable Models for Biased Data</div>
          <!-- Author -->
          <div class="author">
          

          David Madras, <em>Elliot Creager</em>, Toniann Pitassi, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In ACM Conference on Fairness, Accountability, and Transparency</em> Jan 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">madras_fairness_2019</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fairness {Through} {Causal} {Awareness}: {Learning} {Latent}-{Variable} {Models} for {Biased} {Data}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Fairness {Through} {Causal} {Awareness}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Conference on Fairness, Accountability, and Transparency}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/1809.02519}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{arXiv:1809.02519 [cs, stat]}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Machine Learning, Statistics - Machine Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div>

        <!-- Entry bib key -->
        <div id="madras_learning_2018" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Learning Adversarially Fair and Transferable Representations</div>
          <!-- Author -->
          <div class="author">
          

          David Madras*, <em>Elliot Creager*</em>, Toniann Pitassi, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 35th International Conference on Machine Learning</em> Jul 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/1802.06309" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/1802.06309" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/VectorInstitute/laftr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">madras_learning_2018</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning {Adversarially} {Fair} and {Transferable} {Representations}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v80/madras18a.html}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-09-07}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 35th {International} {Conference} on {Machine} {Learning}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Madras*, David and Creager*, Elliot and Pitassi, Toniann and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{ISSN: 2640-3498}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3384--3393}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICLR Workshops</abbr></div>

        <!-- Entry bib key -->
        <div id="grathwohl_gradient-based_2018" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Gradient-Based Optimization Of Neural Network Architecture</div>
          <!-- Author -->
          <div class="author">
          

          Will Grathwohl*, <em>Elliot Creager*</em>, Seyed Kamyar Seyed Ghasemipour*, and Richard Zemel</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In ICLR (Workshop Track)</em> Apr 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openreview.net/pdf?id=HkSm8t1PM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Neural networks can learn relevant features from data, but their predictive accuracy and propensity to overﬁt are sensitive to the values of the discrete hyperparameters that specify the network architecture (number of hidden layers, number of units per layer, etc.). Previous work optimized these hyperparmeters via grid search, random search, and black box optimization techniques such as Bayesian optimization. Bolstered by recent advances in gradient-based optimization of discrete stochastic objectives, we instead propose to directly model a distribution over possible architectures and use variational optimization to jointly optimize the network architecture and weights in one training pass. We discuss an implementation of this approach that estimates gradients via the Concrete relaxation, and show that it ﬁnds compact and accurate architectures for convolutional neural networks applied to the CIFAR10 and CIFAR100 datasets.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">grathwohl_gradient-based_2018</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Gradient}-{Based} {Optimization} {Of} {Neural} {Network} {Architecture}}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Grathwohl*, Will and Creager*, Elliot and Ghasemipour*, Seyed Kamyar Seyed and Zemel, Richard}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR (Workshop Track)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ISMIR</abbr></div>

        <!-- Entry bib key -->
        <div id="creager_nonnegative_2016" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Nonnegative Tensor Factorization with Frequency Modulation Cues for Blind Audio Source Separation</div>
          <!-- Author -->
          <div class="author">
          

          <em>Elliot Creager</em>, Noah D. Stein, Roland Badeau, and Philippe Depalle</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In 17th International Society for Music Information Retrieval Conference</em> Aug 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/1606.00037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/1606.00037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/ecreager/vibntf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present Vibrato Nonnegative Tensor Factorization, an algorithm for single-channel unsupervised audio source separation with an application to separating instrumental or vocal sources with nonstationary pitch from music recordings. Our approach extends Nonnegative Matrix Factorization for audio modeling by including local estimates of frequency modulation as cues in the separation. This permits the modeling and unsupervised separation of vibrato or glissando musical sources, which is not possible with the basic matrix factorization formulation. The algorithm factorizes a sparse nonnegative tensor comprising the audio spectrogram and local frequency-slope-to-frequency ratios, which are estimated at each time-frequency bin using the Distributed Derivative Method. The use of local frequency modulations as separation cues is motivated by the principle of common fate partial grouping from Auditory Scene Analysis, which hypothesizes that each latent source in a mixture is characterized perceptually by coherent frequency and amplitude modulations shared by its component partials. We derive multiplicative factor updates by Minorization-Maximization, which guarantees convergence to a local optimum by iteration. We then compare our method to the baseline on two separation tasks: one considers synthetic vibrato notes, while the other considers vibrato string instrument recordings.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">creager_nonnegative_2016</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nonnegative Tensor Factorization with Frequency Modulation Cues for Blind Audio Source Separation}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/1606.00037}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-11-06}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Creager, Elliot and Stein, Noah D. and Badeau, Roland and Depalle, Philippe}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{arXiv:1606.00037 [cs]}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Sound}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{17th International Society for Music Information Retrieval Conference}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Thesis</abbr></div>

        <!-- Entry bib key -->
        <div id="creager_musical_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Musical Source Separation by Coherent Frequency Modulation Cues</div>
          <!-- Author -->
          <div class="author">
          

          <em>Elliot Creager</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            Aug 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://escholarship.mcgill.ca/downloads/9593tx93q.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This thesis explores the extraction of vibrato sounds from monaural excerpts of polyphonic music using the coherent frequency modulation (CFM) of component partials as a grouping cue. Nonnegative Matrix Factorization (NMF) (Lee and Seung 1999) is currently a popular tool for musical source separation (Wang and Plumbley 2005), since it can provide a lowrank approximate factorization of the magnitude spectrogram of the analyzed sound, where the factors can be interpreted as the spectral templates and temporal activations of the notes contributing to the recording. However, NMF implicitly models each source as having a ﬁxed spectral template and is thus ill-suited to the analysis of vibrato sounds, which are characterized by slowly varying frequency and amplitude modulations.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">creager_musical_2015</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Musical {Source} {Separation} by {Coherent} {Frequency} {Modulation} {Cues}}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{McGill University (Canada)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Creager, Elliot}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Elliot  Creager. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="/assets/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="/assets/js/mdb.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?99f5dd6bf47fe01169a4267f279df989"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?40a752d5efea2d0dae8b9d1dee79d33d"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
